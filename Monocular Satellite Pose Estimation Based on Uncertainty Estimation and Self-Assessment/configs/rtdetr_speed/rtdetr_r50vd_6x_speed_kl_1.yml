
__include__: [
  '../dataset/speed_landmark.yml',
  '../runtime.yml',
  './include/optimizer.yml',
  './include/rtdetr_r50vd.yml',
]

num_classes: 11

checkpoint_step: 12
log_step: 20
sync_bn: False
output_dir: output/rtdetr_r50vd_6x_speed_1_kl
scaler:
  type: GradScaler
  enabled: False

train_dataloader: 
  batch_size: 50
  num_workers: 4
  dataset:
    resize: 256
    #index_file: train_9000_1000/train_1_9000.txt
    index_file: train_1.txt


val_dataloader:
  batch_size: 50
  num_workers: 4
  dataset:
    resize: 256
    index_file: val_1.txt
    #index_file: train_9000_1000/train_1_1000.txt

PResNet:
  depth: 50
  freeze_at: -1
  freeze_norm: False
  pretrained: True

HybridEncoder:
  #in_channels: [128, 256, 512]
  hidden_dim: 256
  expansion: 0.5
  eval_spatial_size: [256, 256]


RTDETRTransformer:
  eval_idx: -1
  num_decoder_layers: 3
  num_denoising: 0
  num_queries: 30
  eval_spatial_size: [256, 256]

SetCriterion:
  weight_dict: {loss_ce: 1, loss_bbox: 5}
  losses: ['labels', 'points_uncert']


optimizer:
  type: AdamW
  params: 
    - 
      params: '^(?=.*backbone)(?=.*norm).*$'
      lr: 0.00003
      weight_decay: 0.
    - 
      params: '^(?=.*backbone)(?!.*norm).*$'
      lr: 0.00003
    - 
      params: '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bias)).*$'
      weight_decay: 0.

  lr: 0.0003
  betas: [0.9, 0.999]
  weight_decay: 0.0001

lr_scheduler:
  type: MultiStepLR
  milestones: [1000]
  gamma: 0.1
